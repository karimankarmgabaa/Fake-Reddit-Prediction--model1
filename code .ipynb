{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74fad27",
   "metadata": {},
   "source": [
    "<h1 style=\"property:value;color:Coral;font-size:300%;text-align:center; \">CISC-873-DM-F22-A3</h1> \n",
    "<h3 style=\"property:value;color:Coral;font-size:200%;text-align:center; \">Fake Reddit Prediction part1</h3> \n",
    "<h5 style=\"property:value;color:Sienna;font-size:150%; \">Kariman Karm Mohamed Mousaa </h5> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693090d2",
   "metadata": {},
   "source": [
    "### 1- Define the problem. What is the input? What is the output? What data mining function is required? What could be the challenges? What is the impact? What is an ideal solution?\n",
    "ans:\n",
    "\n",
    "* problem:we are going to predict Fake Reddit based on the text(news).\n",
    "* input:for train process 2 columns 1 (text) training and 1 (label)target values (match).\n",
    "* output:y_hat for Match, we will prediction if it Fake or not.\n",
    "* data mining function: Classification\n",
    "* the challenges: clean the text and tokenization.\n",
    "* the impact: there many steps for cleaning and ideas in NLP.\n",
    "* an ideal solution: drop all unimportant, stripping text for hyphens and NLP method . \n",
    "----------------------------------------------------------------\n",
    "### 2- What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
    "ans:\n",
    "* character n-grams will need much less storage space and will , subsequently, hold much less information. They are usually utilized in such tasks as language identification, writer identification, anomaly detection.\n",
    "* word n-grams, they may serve the same purposes, and much more, but they need much more storage. For instance, we will need up to several gigabytes to represent in memory a useful subset of English word 3-grams (for general purpose tasks). if we have a limited set of texts we need to work with, word-level n-grams may not require so much storage.\n",
    "the word n-grams suffer more from the OOV issue, to solve such as proplem the n-grams in the corpus that contain an out-of-vocabulary word are ignored.\n",
    "----------------------------------------------------------------------------\n",
    "### 3- What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
    "ans:\n",
    "* stop word:remove all words don't carry the same amount of information, if any information at all, for a predictive modeling task. Common words that carry little (or perhaps no) meaningful information are called stop words like that , is ..... \n",
    "* stemming:the process of identifying the base word (or stem) for a data set of words. Stemming is concerned with the linguistics subfield of morphology, how words are formed. In this example, \"trees\" would lose its letter \"s\" while \"tree\" stays the same. \n",
    "* yes,stop word removal and stemming algorithms are language dependent. \n",
    "-------------------------------------------------------------------------\n",
    "### 4- Is tokenization techniques language dependent? Why?\n",
    "ans:\n",
    "* yes,Tokenization is the process of breaking down a piece of text into small units called tokens. A token may be a word, part of a word or just characters like punctuation. It is one of the most foundational NLP task and a difficult one, because every language has its own grammatical constructs, which are often difficult to write down as rules.\n",
    "-------------------------------------------------------------------------\n",
    "### 5- What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
    "ans:\n",
    "* Count Vectorizer is a way to convert a given set of strings into a frequency representation,Count Vectors can be helpful in understanding the type of text by the frequency of words.\n",
    "* TF-IDF means Term Frequency - Inverse Document Frequency. This is a statistic that is based on the frequency of a word in the corpus but it also provides a numerical representation of how important a word is for statistical analysis.TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words.\n",
    "* not, it's depended on dataset and modeling. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1fc98e",
   "metadata": {},
   "source": [
    "### Import Libararies and Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e44ce04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libararies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21ee3f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In the 1920鈥檚, Hitler was forbidden to address...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nerd Wins Scrabble with word you've never hear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Why 95.8% of Female Newscasters Have the Same ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Donald Trump Says He'll Do This If More 'Inapp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5 crazy facts about Lamborghini's outrageous e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernie Sanders coming to the Walmart sharehold...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>James Corden reveals what he would do if Kanye...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Sorry R2-D2鈥?\"\\t\\t0\\t2\\t4\\nwelashubby\\tscient...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Granddaddy of something EVERYONE takes for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Minnesota: St. Paul launches web portal for Op...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Dem rep says he won't participate in moment of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Woman who loses everything in apartment fire f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Were these stories followed-up on? \"As White N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Oregon Eyewitness Blasts \"White Domestic Terro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"JULIA! WHERE'S YOUR PARACHUTE鈥?\"\\t\\t0\\t2\\t4\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"my wife left me two days ago鈥?\"\\t\\t0\\t2\\t4\\nD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Love This Story of a Day Spent With Maddy Stua...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>World鈥檚 first head transplant volunteer could ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"Fascist Infested!\" 1970 锟?\\t0.97\\t0\\t1\\t5\\nJo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"Oh, don't mind me鈥?\"\\t0.79\\t0\\t2\\t2\\nwuppinda...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A Terrible Ghost, 1944, \"Hitler can鈥檛 sleep. T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What the fuck did you just fucking say about m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>This Was Joe Budden's Response After Quavo Cal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What鈥檚 Behind The 鈥楥uckservative鈥?Slur? -- As ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Oxford Dictionaries鈥?word of the year is a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>H叹蜖虁蛝炭虙蜐蛡蛼虉虌蛦虇蛢蜎虥虈蛫虂叹虆蛺蛢虇虁虜虜蛦探蛻虝虈挞碳蜄台虧坍蛽挞蜌虩态抬瘫...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What the fuck did you just fucking say about m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>A woman in a restaurant complained about my ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>鈾?鈾?*\"Look around you. There are many things t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>鈥淪ay cheese!鈥?\"Wooah!鈥?\\t0.85\\t0\\t2\\t2\\nILikeN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>鈥淚 think it鈥檚 an important time in Mexico-US r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Howdy ya'll, my name is Kim John. I'm a 35 yea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>McCain Says that Trump and Congressional Repub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>T蜎虈虃停虌廷覊蹋蜋虧坦贪瘫蛿袒虦坛滩虦滩h停虂虓桐蛺桐虙虗虙虆虙虉虈彤虖蜔谭虥蛠覊蛪蹋碳蜋...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>\"Our battle has been long and drawn out... We'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Dovakhiin, Dovakhiin, naal ok zin los vahriin ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>**_艇蛻蛡虒坦虧虡虩碳摊虦虡_虂彤蛣停蛢蛺蛫虗虝蜑虩虡-蛬彤瞳庭虛艇蛡瞳虇蛠谈虡蛽虡胎'虄...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NEWS UPDATE: Trump disqualified from Republica...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I know who you are, I know what you want. You ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Warning: House of Cards season 2 Spoiler. Do n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>http://www.google.ca/imgres?um=1&amp;hl=en&amp;safe=of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Martha got home early from work. Her son was i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Next item is... This is item number 4 5 7 8 1 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>*\"Enemies of the Imperium, hear me. You have c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>The assignment was to take the hill. There wer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0   A group of friends began to volunteer at a hom...      0\n",
       "1   British Prime Minister @Theresa_May on Nerve A...      0\n",
       "2   In 1961, Goodyear released a kit that allows P...      0\n",
       "3   Happy Birthday, Bob Barker! The Price Is Right...      0\n",
       "4   Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0\n",
       "5   In the 1920鈥檚, Hitler was forbidden to address...      0\n",
       "6   Nerd Wins Scrabble with word you've never hear...      0\n",
       "7   Why 95.8% of Female Newscasters Have the Same ...      1\n",
       "8   Donald Trump Says He'll Do This If More 'Inapp...      0\n",
       "9   5 crazy facts about Lamborghini's outrageous e...      0\n",
       "10  Bernie Sanders coming to the Walmart sharehold...      0\n",
       "11  James Corden reveals what he would do if Kanye...      0\n",
       "12  \"Sorry R2-D2鈥?\"\\t\\t0\\t2\\t4\\nwelashubby\\tscient...      0\n",
       "13  The Granddaddy of something EVERYONE takes for...      1\n",
       "14  Minnesota: St. Paul launches web portal for Op...      0\n",
       "15  Dem rep says he won't participate in moment of...      0\n",
       "16  Woman who loses everything in apartment fire f...      0\n",
       "17  Were these stories followed-up on? \"As White N...      1\n",
       "18  Oregon Eyewitness Blasts \"White Domestic Terro...      0\n",
       "19  \"JULIA! WHERE'S YOUR PARACHUTE鈥?\"\\t\\t0\\t2\\t4\\n...      0\n",
       "20  \"my wife left me two days ago鈥?\"\\t\\t0\\t2\\t4\\nD...      0\n",
       "21  Love This Story of a Day Spent With Maddy Stua...      0\n",
       "22  World鈥檚 first head transplant volunteer could ...      0\n",
       "23  \"Fascist Infested!\" 1970 锟?\\t0.97\\t0\\t1\\t5\\nJo...      0\n",
       "24  \"Oh, don't mind me鈥?\"\\t0.79\\t0\\t2\\t2\\nwuppinda...      0\n",
       "25  A Terrible Ghost, 1944, \"Hitler can鈥檛 sleep. T...      0\n",
       "26  What the fuck did you just fucking say about m...      0\n",
       "27  This Was Joe Budden's Response After Quavo Cal...      0\n",
       "28  What鈥檚 Behind The 鈥楥uckservative鈥?Slur? -- As ...      1\n",
       "29  The Oxford Dictionaries鈥?word of the year is a...      0\n",
       "30  H叹蜖虁蛝炭虙蜐蛡蛼虉虌蛦虇蛢蜎虥虈蛫虂叹虆蛺蛢虇虁虜虜蛦探蛻虝虈挞碳蜄台虧坍蛽挞蜌虩态抬瘫...      0\n",
       "31  What the fuck did you just fucking say about m...      0\n",
       "32  A woman in a restaurant complained about my ki...      0\n",
       "33  鈾?鈾?*\"Look around you. There are many things t...      1\n",
       "34  鈥淪ay cheese!鈥?\"Wooah!鈥?\\t0.85\\t0\\t2\\t2\\nILikeN...      0\n",
       "35  鈥淚 think it鈥檚 an important time in Mexico-US r...      0\n",
       "36  Howdy ya'll, my name is Kim John. I'm a 35 yea...      0\n",
       "37  McCain Says that Trump and Congressional Repub...      1\n",
       "38  T蜎虈虃停虌廷覊蹋蜋虧坦贪瘫蛿袒虦坛滩虦滩h停虂虓桐蛺桐虙虗虙虆虙虉虈彤虖蜔谭虥蛠覊蛪蹋碳蜋...      0\n",
       "39  \"Our battle has been long and drawn out... We'...      0\n",
       "40  Dovakhiin, Dovakhiin, naal ok zin los vahriin ...      0\n",
       "41  **_艇蛻蛡虒坦虧虡虩碳摊虦虡_虂彤蛣停蛢蛺蛫虗虝蜑虩虡-蛬彤瞳庭虛艇蛡瞳虇蛠谈虡蛽虡胎'虄...      0\n",
       "42  NEWS UPDATE: Trump disqualified from Republica...      0\n",
       "43  I know who you are, I know what you want. You ...      0\n",
       "44  Warning: House of Cards season 2 Spoiler. Do n...      0\n",
       "45  http://www.google.ca/imgres?um=1&hl=en&safe=of...      0\n",
       "46  Martha got home early from work. Her son was i...      0\n",
       "47  Next item is... This is item number 4 5 7 8 1 ...      0\n",
       "48  *\"Enemies of the Imperium, hear me. You have c...      0\n",
       "49  The assignment was to take the hill. There wer...      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data \n",
    "train=pd.read_csv('xy_train.csv')\n",
    "test=pd.read_csv('x_test.csv')\n",
    "sample=pd.read_csv('sample_submission.csv')\n",
    "ID=sample['id']\n",
    "#drop id \n",
    "train.drop('id',axis=1,inplace=True)\n",
    "test.drop('id',axis=1,inplace=True)\n",
    "train.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc4178",
   "metadata": {},
   "source": [
    "## Exploring data and Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bbba1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    60000 non-null  object\n",
      " 1   label   60000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#information about train data \n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "265ffdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41a361ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.467667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.506648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  60000.000000\n",
       "mean       0.467667\n",
       "std        0.506648\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        2.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#desribe the data\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fc35f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to x,y\n",
    "x=train['text']\n",
    "y=train['label']\n",
    "test=test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4eaee9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     a group of friends began to volunteer at a hom...\n",
       "1     british prime minister theresamay on nerve att...\n",
       "2     in goodyear released a kit that allows to be b...\n",
       "3     happy birthday bob barker the price is right h...\n",
       "4     obama to nation 聙innocent cops and unarmed you...\n",
       "5     in the hitler was forbidden to address public ...\n",
       "6     nerd wins scrabble with word youve never heard...\n",
       "7     why of female newscasters have the same hair  ...\n",
       "8     donald trump says hell do this if more inappro...\n",
       "9     crazy facts about lamborghinis outrageous elec...\n",
       "10    bernie sanders coming to the walmart sharehold...\n",
       "11    james corden reveals what he would do if kanye...\n",
       "12    sorry welashubby scientists discover that fart...\n",
       "13    the granddaddy of something everyone takes for...\n",
       "14    minnesota st paul launches web portal for oppo...\n",
       "15    dem rep says he wont participate in moment of ...\n",
       "16    woman who loses everything in apartment fire f...\n",
       "17    were these stories followedup on as white nati...\n",
       "18    oregon eyewitness blasts white domestic terror...\n",
       "19    julia wheres your parachute鈥  frenchymanfella ...\n",
       "20    my wife left me two days ago鈥  s national soci...\n",
       "21    love this story of a day spent with maddy stua...\n",
       "22    world鈥檚 first head transplant volunteer could ...\n",
       "23    fascist infested 锟 jodieboo husky and a rubber...\n",
       "24    oh dont mind me鈥 wuppindalsa cookie monster ex...\n",
       "25    a terrible ghost hitler can鈥檛 sleep through th...\n",
       "26    what the fuck did you just fucking say about m...\n",
       "27    this was joe buddens response after quavo call...\n",
       "28    what鈥檚 behind the 鈥楥uckservative鈥slur  as eric...\n",
       "29    the oxford dictionaries鈥word of the year is a ...\n",
       "30    h叹蜖虁蛝炭虙蜐蛡蛼虉虌蛦虇蛢蜎虥虈蛫虂叹虆蛺蛢虇虁虜虜蛦探蛻虝虈挞碳蜄台虧坍蛽挞蜌虩态抬瘫...\n",
       "31    what the fuck did you just fucking say about m...\n",
       "32    a woman in a restaurant complained about my ki...\n",
       "33    鈾鈾look around you there are many things to see...\n",
       "34    鈥淪ay cheese鈥wooah鈥 ilikeneurons gop slowly piv...\n",
       "35    鈥淚 think it鈥檚 an important time in mexicous re...\n",
       "36    howdy yall my name is kim john im a year old k...\n",
       "37    mccain says that trump and congressional repub...\n",
       "38    t蜎虈虃停虌廷覊蹋蜋虧坦贪瘫蛿袒虦坛滩虦滩h停虂虓桐蛺桐虙虗虙虆虙虉虈彤虖蜔谭虥蛠覊蛪蹋碳蜋...\n",
       "39    our battle has been long and drawn out weve su...\n",
       "40    dovakhiin dovakhiin naal ok zin los vahriin wa...\n",
       "41    艇蛻蛡虒坦虧虡虩碳摊虦虡虂彤蛣停蛢蛺蛫虗虝蜑虩虡蛬彤瞳庭虛艇蛡瞳虇蛠谈虡蛽虡胎虄亭叹虌坦贪蛵...\n",
       "42    news update trump disqualified from republican...\n",
       "43    i know who you are i know what you want you ar...\n",
       "44    warning house of cards season spoiler do not c...\n",
       "45                                                     \n",
       "46    martha got home early from work her son was in...\n",
       "47    next item is this is item number look at these...\n",
       "48    enemies of the imperium hear me you have come ...\n",
       "49    the assignment was to take the hill there were...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Libraries regular expression and string \n",
    "import re\n",
    "import string\n",
    "#function for clean text \n",
    "#in trail1,2,3 \n",
    "def clean_text(text):\n",
    "    # covert the text to lowercase\n",
    "    text = text.lower()  \n",
    "    # remove html chars\n",
    "    text = re.sub('<.*?>','',text).strip() \n",
    "    # remove text in square brackets and parenthesis\n",
    "    text = re.sub('\\[|\\(.*\\]|\\)','', text).strip() \n",
    "     # remove punctuation marks\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # remove non-ascii chars\n",
    "    text = re.sub(\"(\\\\W)\",\" \",text).strip() \n",
    "    # remove words containing numbers\n",
    "    text = re.sub('\\S*\\d\\S*\\s*','', text).strip()  \n",
    "    #remove non english characters \n",
    "    #text =  re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "    return text\n",
    "#covert type of series to string and apply the function for each column\n",
    "x = x.astype(str)\n",
    "x = x.apply(clean_text)\n",
    "test=test.astype(str)\n",
    "test=test.apply(clean_text)\n",
    "x.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a19f0f",
   "metadata": {},
   "source": [
    "## --> in trial 1,2,3,we note in the result about the cell above there are non english text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b715f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install libraries will need it in NLP\n",
    "#pip install xgboost\n",
    "#pip install scikit-multilearn \n",
    "#pip install nltk\n",
    "#pip install setuptools wheel\n",
    "#pip install spacy\n",
    "#pip install wordcloud \n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfc41f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "091d98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the english language small model of spacy to covert all text to english\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#x = x.astype(str)\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6fb7387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    group friend began volunt homeless shelter nei...\n",
       "1    british prime minist theresamay nerv attack ru...\n",
       "2    goodyear releas kit allow brought heel 鈥 fish ...\n",
       "3    happi birthday bob barker price right host hed...\n",
       "4    obama nation 聙innoc cop unarm young black men ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function remove stop words from a text.\n",
    "def apply_stemmer(text):\n",
    "    words = text.split()\n",
    "    sent = [snow_stemmer.stem(word) for word in words if not word in set(stopwords)]\n",
    "    return ' '.join(sent)\n",
    "# remove the stop words from x and test\n",
    "x = x.apply(apply_stemmer)\n",
    "test=test.apply(apply_stemmer)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "646bd071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "like        3086\n",
       "new         2986\n",
       "look        2836\n",
       "color       2622\n",
       "man         2596\n",
       "trump       2370\n",
       "peopl       2257\n",
       "found       1994\n",
       "time        1951\n",
       "poster      1900\n",
       "day         1806\n",
       "war         1767\n",
       "work        1498\n",
       "say         1487\n",
       "help        1440\n",
       "world       1430\n",
       "life        1393\n",
       "american    1385\n",
       "old         1372\n",
       "state       1348\n",
       "school      1313\n",
       "photo       1308\n",
       "save        1295\n",
       "know        1250\n",
       "hous        1224\n",
       "want        1214\n",
       "circa       1211\n",
       "presid      1206\n",
       "right       1201\n",
       "psbattl     1164\n",
       "woman       1161\n",
       "pictur      1158\n",
       "way         1130\n",
       "true        1120\n",
       "got         1119\n",
       "find        1100\n",
       "get         1094\n",
       "polic       1080\n",
       "dog         1067\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.models import NumeralTickFormatter\n",
    "# Word Frequency of most common words\n",
    "word_freq = pd.Series(\" \".join(x).split()).value_counts()\n",
    "word_freq[1:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a24a1f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>鈥淕rosvenor鈥metro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>circumfer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wbush</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>homefront</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>congol茅opoldvill</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>napoleon鈥檚</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>out鈥</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>soborova</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tapes鈥a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index  freq\n",
       "0  鈥淕rosvenor鈥metro     1\n",
       "1         circumfer     1\n",
       "2              aang     1\n",
       "3             wbush     1\n",
       "4         homefront     1\n",
       "5  congol茅opoldvill     1\n",
       "6        napoleon鈥檚     1\n",
       "7              out鈥     1\n",
       "8          soborova     1\n",
       "9           tapes鈥a     1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list most uncommon words\n",
    "word_freq[-10:].reset_index(name=\"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fad159d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dad vietnam didnt know photo exist came random internet cri saw hope right place post             0.000433\n",
       "rfakehistoryporn subscrib attempt read seri basic instruct                                        0.000367\n",
       "dad vietnam didn鈥檛 know photo exist came random internet cri saw hope right place post            0.000300\n",
       "                                                                                                  0.000133\n",
       "new evid confirm human ancestor climb tree retriev drop snack                                     0.000050\n",
       "                                                                                                    ...   \n",
       "warrior attribut final loss durant鈥檚 ruptur achill klay鈥檚 torn acl curri hit bus near end game    0.000017\n",
       "peopl walk brother post peopl add thing wed album help epic                                       0.000017\n",
       "ed gein butcher plainsfield pictur mask skin femal head circa                                     0.000017\n",
       "german soldier enjoy rare christma meal sent ruin stalingrad januari color                        0.000017\n",
       "jeff bridg releas 鈥楽leep tapes鈥a new album design help fall asleep                                0.000017\n",
       "Name: text, Length: 59563, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of ratings\n",
    "x.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d35cb0",
   "metadata": {},
   "source": [
    "### we can see in above result there are non english letter in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e873404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.3, min_df=10, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform each sentence to numeric vector with tf-idf value as elements\n",
    "\n",
    "\"\"\"\n",
    "Compute unique word vector with frequencies\n",
    "exclude very uncommon (<10 obsv.) and common (>=30%) words\n",
    "use pairs of two words (ngram)\n",
    "\"\"\"\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"word\", max_df=0.3, min_df=10, ngram_range=(1, 2), norm=\"l2\"\n",
    ")\n",
    "# execute in x and test\n",
    "vectorizer.fit(x)\n",
    "vectorizer.fit(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24bfc1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# Sample data - 20% of data to validation set\n",
    "x_train,x_val,y_train,y_val = train_test_split(x,y, random_state=1, test_size=0.2, shuffle=True)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3df897e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique word (ngram) vector extract:\n",
      "\n",
      " water droplet    4737\n",
      "care              620\n",
      "autumn            241\n",
      "collect           826\n",
      "contact           906\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# transform each sentence to numeric vector with tf-idf value as elements\n",
    "X_train_vec = vectorizer.transform(x_train)\n",
    "X_val_vec = vectorizer.transform(x_val)\n",
    "test_vec=vectorizer.transform(test)\n",
    "X_train_vec.get_shape()\n",
    "# Vector representation of vocabulary with sample=5 and random_state=1\n",
    "word_vector = pd.Series(vectorizer.vocabulary_).sample(5, random_state=1)\n",
    "print(f\"Unique word (ngram) vector extract:\\n\\n {word_vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaca8bb",
   "metadata": {},
   "source": [
    "## LogisticRegression Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3624fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.7710833333333333\n"
     ]
    }
   ],
   "source": [
    "#trial 1\n",
    "#define MODEL with c=10,L2 regularization  and random_state=100\n",
    "logistic_regression = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=100)\n",
    "# execute model\n",
    "logistic_regression.fit(X_train_vec, y_train)\n",
    "# predict the model\n",
    "y_pred = logistic_regression.predict(X_val_vec)\n",
    "print(\"train accuracy: \",accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e567e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] =ID\n",
    "submission['label'] = logistic_regression.predict_proba(test_vec)[:,1]\n",
    "submission.to_csv('tex1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4aefe",
   "metadata": {},
   "source": [
    "## DecisionTree Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "201c5b42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter C for estimator DecisionTreeClassifier(). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SAIFTO~1\\AppData\\Local\\Temp/ipykernel_6260/511723258.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdtree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# execute dtree with search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# predict the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    584\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                 raise ValueError('Invalid parameter %s for estimator %s. '\n\u001b[0m\u001b[0;32m    231\u001b[0m                                  \u001b[1;34m'Check the list of available parameters '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                                  \u001b[1;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter C for estimator DecisionTreeClassifier(). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "#trial2\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#define MODEL with random_state=100\n",
    "dtree= DecisionTreeClassifier()\n",
    "# DEFINE PARAMETER c take values logspace(-3,3,7) and penalty  l2 ridge\n",
    "grid={\"C\":np.logspace(-3,3,20), \"penalty\":[\"l2\"],'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n",
    "# define search\n",
    "dtree=GridSearchCV(dtree,grid)\n",
    "# execute dtree with search\n",
    "dtree.fit(X_train_vec, y_train)\n",
    "# predict the model\n",
    "y_pred = dtree.predict(X_val_vec)\n",
    "print(\"train accuracy: \",accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ffefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] =ID\n",
    "submission['label'] = dtree.predict_proba(test_vec)[:,1]\n",
    "submission.to_csv('tex2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49861c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trail4 \n",
    "# we make turning with LogisticRegression\n",
    "# DEFINE PARAMETER c take values logspace(-3,3,7) and penalty  l2 ridge\n",
    "grid={\"C\":np.logspace(-3,3,20), \"penalty\":[\"l2\"],'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n",
    "grid_search=GridSearchCV(logistic_regression,grid)\n",
    "# execute search\n",
    "grid_search.fit(X_train_vec, y_train)\n",
    "# predict the model\n",
    "y_pred = grid_search.predict(X_val_vec)\n",
    "print(\"train accuracy: \",accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] =ID\n",
    "submission['label'] = grid_search.predict_proba(test_vec)[:,1]\n",
    "submission.to_csv('tex4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232d7c1",
   "metadata": {},
   "source": [
    "## Random Forest Modeling  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f975981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trail3 \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#define MODEL with max_depth=100 ,random_state=20\n",
    "clf = RandomForestClassifier(max_depth=100, random_state=20)\n",
    "# DEFINE PARAMETER c take values logspace(-3,3,7) and penalty  l2 ridge\n",
    "grid={\"C\":np.logspace(-3,3,20), \"penalty\":[\"l2\"],'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n",
    "# define search\n",
    "clf=GridSearchCV(clf,grid)\n",
    "# execute model\n",
    "clf.fit(X_train_vec, y_train)\n",
    "# predict the model\n",
    "y_pred = clf.predict(X_val_vec)\n",
    "print(\"train accuracy: \",accuracy_score(y_pred, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] =ID\n",
    "submission['label'] = clf.predict_proba(test_vec)[:,1]\n",
    "submission.to_csv('tex3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92edca5",
   "metadata": {},
   "source": [
    "## result for test trial1,2,3: 0.81839, 0.69751,0.79938\n",
    "### we note LogisticRegression is the best model in trial 1,2,3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] =ID\n",
    "submission['label'] = grid_search.predict_proba(test_vec)[:,1]\n",
    "submission.to_csv('tex4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c081c59",
   "metadata": {},
   "source": [
    "## XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba59245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial 5\n",
    "from xgboost import XGBClassifier\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "\n",
    "# DEFINE PARAMETER c take values logspace(-3,3,7) and penalty  l2 ridge\n",
    "grid={\"C\":np.logspace(-3,3,20), \"penalty\":[\"l2\"],'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n",
    "model=GridSearchCV(model,grid)\n",
    "# execute search\n",
    "model.fit(X_train_vec, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_val_vec)\n",
    "print(\"train accuracy: \",accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1faae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] =ID\n",
    "submission['label'] = model.predict_proba(test_vec)[:,1]\n",
    "submission.to_csv('tex5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70289d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
